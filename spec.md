# Coding Guidelines

* Do your best not to create multiple exported functions or types with similar
  names. For example `Foo` and `FooWithContext`, etc. The exported API should be unmistakeably simple to find. Unexported functions have no such limitation. Also, Try your best not introduce things like Compile/CompileSchema, etc

# Builder

The builder is automatically generated by internal/cmd/genobjects/main.go.
You will need to update this file and then call ./gen.sh to regenerate the builder.

IMPORTANT: You may NEVER modify any file with _gen.go suffix directly. These files
must be modified through modifying the generator

# Clone Builder

Normally a builder starts with a fresh empty Schema. There should be a way to
create a Builder with a pre-initialized Schema object. 


```
original := ... // Schema object
schema.NewBuilder().Clone(original)
```

This should allow "Resetting" some fields when necessary. For example, if you
processed "$ref" and wanted to process the rest of the Schema, you could do this
to create an identical schema, but without the $ref field:

```
original := ...
s, err := schema.NewBuilder().Clone(original).ResetReference().MustBuild()
```

# Schema

Due to the number of fields that need to be checked with HasXXX() calls, the Schema object should have a bit field to hold which fields have been set or not.

This allows for a construct like

```
if s.HasAnchor() && s.HasDefinitions() && s.HasProperties() {
  ...
}
```

to be turned into

```
requiredFields := schema.AnchorField | schema.DefinitionsField | schema.PropertiesField
if s.Has(requiredFields) {
  ...
}
```

which is more efficient.

However, because these fields are used from multiple inter-dependent packages, we will need
to declare them in an internal package first:

```
// in internal/field
package field

const (
    Anchor = ...
    Definitions = ...
    Properties = ...
    ...
)
```

Then, for the end-users, these should be exported by creating new constants in schema package:

```
package schema

import "github.com/lestrrat-go/json-schema/internal/field"

const (
    AnchorField = field.Anchor
    DefinitionsField = field.Definitions
    PropertiesField = field.Properties
    ...
)
```

Code in this module (excluding tests) should reference these by their internal
identifiers.

# Schemas and boolean Schemas

There exists fields where they expect either a boolean or a Schema object.

These are extremely tricky to handle in Go because of typing problems.

To that end, we should define these:

```
type SchemaOrBool interface {
    schemaOrBool() // internal identifier
}

type SchemaBool bool // represents a boolean value in allOf, oneOf, etc
type Schema struct { ... } // a real Schema.
```

`SchemaBool` should be a simple struct that knows how to Marshal to/Unmarshal from
JSON.

```
func (s *SchemaBool) UnmarshalJSON(data []byte) error {
    var b bool
    if err := json.Unmarshal(data, &b); err != nil {
        return ...
    }
    *s = SchemaBool(b)
    return nil
}
```

To unmarshal values for fields like `allOf` et al you would need to start from the
container level, and use a Token based parsing.

```
// This is pseudocode
// assume we're parsing allOf field, and we have the slice data in []data.

dec := json.NewDecoder(bytes.NewReader(data))
for dec.More() {
    tok, err := dec.Token()

    if b, ok := tok.(bool); ok {
        anyOfFields = append(anyOfFields, SchemaOrBool(b))
        continue
    }

    if d, ok := tok.(json.Delim); ok {
        // parse the object.
        ...
    }

    // otherwise it's an error
}
```

For convenience, there should be two functions SchemaTrue() and SchemaFalse() that return the respective SchemaBool objects.

```
var schemaTrue = SchemaBool(true)
var schemaFalse = SchemaFalse(false)

func SchemaTrue() SchemaBool {
    return schemaTrue
}

func SchemaFalse() SchemaBool {
    return schemaFalse
}
```

# Validation

Validation is done by Validator objects, defined in the validator package.

```
package validator

type Interface interface {
    Validate(context.Context, any) (Result, error)
}

type Result any
```

## Result Objects

The only times a result is required is when the resource being validated is either an array or an object.
So there exists `type ObjectResult` and `type ArrayResult`. They should have corresponding constructors
`NewObjectResult()` and `NewArrayResult(size int)`. Fields on these results should be unexported.

There should be a utility function that is used to merge multiple results. When all the results are merged,
it should assign the final result to `dst` using `blackmagic.AssignIfCompatible`.
The type of `dst` dictates how the merging is performed. I.e., if `dst` is of type `*ObjectResult`,
it should expect `*ObjectResult` in `list`, and `*ArrayResult` if `dst` is of type `*ArrayResult`

```
func MergeResults(dst Result, list ...Result) error { ... }
```

`ObjectResult` and `ArrayResult` should NOT implement their own merge methods. Instead there should be
helper functions `func mergeObjectResults(obj1, obj2 *ObjectResult) error` and `func mergeArrayResults(arr1, arr2 *ArrayResult) error`.

## Context passing during Validation

There are cases such as for unevaluatedProperties handling, you need to pass context to/from callees and callers
of the Validate() method.

To get data out of a sub-validator, you use the Result object from the return value of the validation.
For those validators that do not (need to) support this, they should always return a nil Result.

To pass on data to a sub-validator, you use the context.Context argument to pass a validator.Stash object.

```
ctx = validator.WithStash(ctx, &Stash{ ... })
subv.Validate(ctx, ...)
```

```
st := validator.StashFromContext(ctx) // returns nil if no stash is associated with ctx
```

# Composite Validators

During compilation of a validator, the spec could specify a $ref along with other specifications.
In this case, the resulting validator should be a composite validator that checks the incoming
data against the validator compiled from the reference, and the validator compiled from the
rest of the spec.

The composite validator should combine their results and return as a single Result object.
For this reason, 

## allOf/anyOf/oneOf

These composite validators only differ in the success status. For example, given N
schemas for each of the above,

| Name   | Required Number of successful validators |
|--------|------------------------------------------|
| allOf  | == N                                     |
| anyOf  | >  1                                     |
| oneOf  | == 1                                     |

Therefore the implementation can mostly be the same. They just need different logic
for evaluation of a successful validation.


# Reference Resolution

References can potentially be circular, infinitely from the spec. But obviously this module
should not go into an inifite loop. This module should both strive to
_not_ resolve references until when it is absolutely necessary, and also to keep track
of which references have already been resolved, and reuse the values if they appear more
than once.

References in this library should be resolved separate from anchors.
There should be an exported API to resolve only JSON references.
It's okay to have a unified API on top of these to automatically dispatch to the
correct resolver depending on the string.

# Anchor Resolution

Anchors are unique IDs within a JSON Schema document. You can refer to it from anywhere
with an anchor. Anchors must be unique within a document.

Anchors in this library should be resolved separately from references.
There should be an exported API to resolve only anchors.
It's okay to have a unified API on top of these to automatically dispatch to the
correct resolver depending on the string.

# Dependen Schema handling

When dependentSchemas is used like the following,

```
{
    "properties": {
        "foo": { ... }
    },
    "dependentSchemas": {
        "foo": { ... }
    }
}
```

The dependentSchemas should be compiled to validators during initial compilation phase,
and stored in the validator object for later re-use during the execution phase.
During execution phase, the the compiled validatiors should be passed in the context.Context object.
If such data exists in the context, the validator should effectively make two passes
through the properties: one for the "properties" field, and another for "dependentSchemas"

Validation for dependentSchemas should return a Result object that records which properties have
been evaluated. These results, if necessary, should be combined with other Results from, for example,
evaluating against "properties" field.

# Testing

## Meta Schema Testing.

Meta Schema Testing requires that we fetch a live document from the web, but causing unnecessary external network traffic.

So we create a testdata/schemas/meta directory that contains all the necessary files in html/template format. They are in template format so that we can replace the $id field (and any others, if applicable) with the URL of the local test HTTP server.

The test server should be instantiated in the test file, using embed.FS

```
//go:embed testdata/schemas
var metaSchema embed.FS

server := httptest.NewUntestedServer(nil)

mux := http.NewServeMux()
mux.HandleFunc("/", func(...) {
    // Use metaSchema embed.FS to serve files
})
server.Config.Handler = mux
```